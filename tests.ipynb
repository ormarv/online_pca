{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reduced_ipca import ReducedPCA\n",
    "from perturb_ipca import PerturbPCA\n",
    "from utils import dict2numpy, vec2dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check using gaussians "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5.220565644295931e-14, 1: 0.025878167476123367, 2: 0.011633957493537177, 3: 0.12293468327390743, 4: 0.12908795940825354, 5: 5.0955160538929185, 6: 5.9543034507849475, 7: 7.027052494195294, 8: 7.995053166631833, 9: 9.044772715368909}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "test_data = np.random.normal(size=(10_000,10))*np.arange(10)\n",
    "test_data = [{idx: i  for idx, i in enumerate(test_data[i])} for i in range(10_000)]\n",
    "test_vec = {idx: i  for idx, i in enumerate(np.arange(10))} \n",
    "ipca = PerturbPCA(5)\n",
    "for i in test_data:\n",
    "    ipca.learn_one(i)\n",
    "print(ipca.inverse_transform_one(ipca.transform_one(test_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.0, 1: 0.025599074602075535, 2: 0.01266575746956531, 3: 0.12288722014933189, 4: 0.2844437990407748, 5: 5.18922495580125, 6: 5.970517303933131, 7: 7.023804221989852, 8: 7.992712204209254, 9: 9.046572082523122}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "test_data = np.random.normal(size=(10_000,10))*np.arange(10)\n",
    "test_data = [{idx: i  for idx, i in enumerate(test_data[i])} for i in range(10_000)]\n",
    "test_vec = {idx: i  for idx, i in enumerate(np.arange(10))} \n",
    "ipca = ReducedPCA(5)\n",
    "for i in test_data:\n",
    "    ipca.learn_one(i)\n",
    "print(ipca.inverse_transform_one(ipca.transform_one(test_vec)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the IRIS dataset "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We do IRIS dataset clustering using our Reduced PCA algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from river.stream import iter_pandas\n",
    "# unused but required import for doing 3d projections with matplotlib < 3.2\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "print(type(X))\n",
    "y = iris.target\n",
    "stream = list(iter_pandas(X,y))\n",
    "online_pca = ReducedPCA(2)\n",
    "for (x,y) in stream:\n",
    "    online_pca.learn_one(x)\n",
    "    \n",
    "results = np.array([dict2numpy(online_pca.transform_one(x)) for (x,y) in stream])\n",
    "plt.scatter(results[:, 0], results[:, 1],\n",
    "            c=iris.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('gnuplot', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We do IRIS dataset clustering using our PerturbPCA algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from river.stream import iter_pandas\n",
    "# unused but required import for doing 3d projections with matplotlib < 3.2\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "print(type(X))\n",
    "y = iris.target\n",
    "stream = list(iter_pandas(X,y))\n",
    "online_pca = PerturbPCA(2)\n",
    "for (x,y) in stream:\n",
    "    online_pca.learn_one(x)\n",
    "    \n",
    "results = np.array([dict2numpy(online_pca.transform_one(x)) for (x,y) in stream])\n",
    "plt.scatter(results[:, 0], results[:, 1],\n",
    "            c=iris.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('gnuplot', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We do IRIS dataset clustering using Sk-learn PCA algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "iris = datasets.load_iris(as_frame=True)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "sk_pca = PCA(2)\n",
    "sk_pca.fit(X)\n",
    "results = sk_pca.transform(X)\n",
    "    \n",
    "results = np.array([dict2numpy(online_pca.transform_one(x)) for (x,y) in stream])\n",
    "plt.scatter(results[:, 0], results[:, 1],\n",
    "            c=iris.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('gnuplot', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### River intergration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "from river import metrics\n",
    "import pandas as pd \n",
    "from river.stream import iter_pandas\n",
    "\n",
    "def evaluate(stream, model, n_wait=100, verbose=True):\n",
    "    def print_progress(sample_id, acc, kappa):\n",
    "        print(f'Samples processed: {sample_id}')\n",
    "        print(acc)\n",
    "        print(kappa)\n",
    "\n",
    "    acc = metrics.Accuracy()\n",
    "    acc_rolling = metrics.Rolling(metric=metrics.Accuracy(), window_size=n_wait)\n",
    "    kappa = metrics.CohenKappa()\n",
    "    kappa_rolling = metrics.Rolling(metric=metrics.CohenKappa(), window_size=n_wait)\n",
    "    raw_results = []\n",
    "    model_name = model.__class__.__name__\n",
    "    for i, (x, y) in enumerate(stream):\n",
    "        # Predict\n",
    "        y_pred = model.predict_one(x)\n",
    "        # Update metrics and results\n",
    "        acc.update(y_true=y, y_pred=y_pred)\n",
    "        acc_rolling.update(y_true=y, y_pred=y_pred)\n",
    "        kappa.update(y_true=y, y_pred=y_pred)\n",
    "        kappa_rolling.update(y_true=y, y_pred=y_pred)\n",
    "        if i % n_wait == 0 and i > 0:\n",
    "            if verbose:\n",
    "                print_progress(i, acc, kappa)\n",
    "            raw_results.append([model_name, i, acc.get(), acc_rolling.get(), kappa.get(), kappa_rolling.get()])\n",
    "        model.learn_one(x, y)\n",
    "    print_progress(i, acc, kappa)\n",
    "    return pd.DataFrame(raw_results, columns=['model', 'id', 'acc', 'acc_roll', 'kappa', 'kappa_roll'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype\n",
    "data = fetch_covtype(as_frame=True)\n",
    "X ,Y = data.data[:5_000], data.target[:5_000]\n",
    "stream  = iter_pandas(X,Y)\n",
    "pca = PerturbPCA(10)\n",
    "lin = linear_model.SoftmaxRegression()\n",
    "model = pca | lin\n",
    "evaluate(stream=iter_pandas(X=X, y=Y),\n",
    "                      model=model, n_wait=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X ,Y = data.data[:5_000], data.target[:5_000]\n",
    "stream  = iter_pandas(X,Y)\n",
    "pca = ReducedPCA(10)\n",
    "lin = linear_model.SoftmaxRegression()\n",
    "model = pca | lin\n",
    "evaluate(stream=iter_pandas(X=X, y=Y),\n",
    "                      model=model, n_wait=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6adea5f9266a5e589bc8b4a6ecca60a9dcc76615c50e1115b39c5a3d23612a2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
